{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 29d099d7-f56f-445f-a086-3281cdf53933\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 1 define Settings",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%connections hudi-connection\n%glue_version 3.0\n%region us-west-2\n%worker_type G.1X\n%number_of_workers 3\n%spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n%additional_python_modules Faker",
			"metadata": {
				"trusted": true
			},
			"execution_count": 64,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session bb7031ff-b083-41eb-8e2e-450bd249b134.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Connections to be included:\nhudi-connection\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session bb7031ff-b083-41eb-8e2e-450bd249b134.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session bb7031ff-b083-41eb-8e2e-450bd249b134.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous region: us-west-2\nSetting new region to: us-west-2\nReauthenticating Glue client with new region: us-west-2\nIAM role has been set to arn:aws:iam::043916019468:role/Lab3. Reauthenticating.\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nAuthentication done.\nRegion is set to: us-west-2\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session bb7031ff-b083-41eb-8e2e-450bd249b134.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session bb7031ff-b083-41eb-8e2e-450bd249b134.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 3\nSetting new number of workers to: 3\nPrevious Spark configuration: spark.serializer=org.apache.spark.serializer.KryoSerializer\nSetting new Spark configuration to: spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session bb7031ff-b083-41eb-8e2e-450bd249b134.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Additional python modules to be included:\nFaker\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 2: Define Imports",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "\ntry:\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    from pyspark.sql.session import SparkSession\n    from awsglue.dynamicframe import DynamicFrame\n    from pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\n    from pyspark.sql.functions import *\n    from awsglue.utils import getResolvedOptions\n    from awsglueml.transforms import EntityDetector\n    from pyspark.sql.types import StringType\n    from pyspark.sql.types import *\n    from datetime import datetime\n\n    import boto3\n    from functools import reduce\nexcept Exception as e:\n    print(\"Error \")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 3: Create Spark Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder.config('spark.serializer','org.apache.spark.serializer.KryoSerializer') \\\n                            .config('spark.sql.hive.convertMetastoreParquet','false') \\\n                            .config('spark.sql.legacy.pathOptionBehavior.enabled', 'true') .getOrCreate()\n\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 55,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 4: Generating PII data and Inserting into Hudi Datalakes ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import uuid\nfrom faker import Faker\n\nglobal faker\nfaker = Faker()\n\nclass DataGenerator(object):\n\n    @staticmethod\n    def get_data():\n        return [\n            (\n                uuid.uuid4().__str__(),\n                faker.name(),\n                faker.random_element(elements=('IT', 'HR', 'Sales', 'Marketing')),\n                faker.random_element(elements=('CA', 'NY', 'TX', 'FL', 'IL', 'RJ')),\n                str(faker.random_int(min=10000, max=150000)),\n                str(faker.random_int(min=18, max=60)),\n                str(faker.random_int(min=0, max=100000)),\n                str(faker.unix_time()),\n                faker.email(),\n                faker.credit_card_number(card_type='amex')\n            ) for x in range(20)\n        ]\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 63,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data = DataGenerator.get_data()\ncolumns = [\"emp_id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\", \"ts\", \"email\", \"credit_card\"]\nspark_df = spark.createDataFrame(data=data, schema=columns)\nspark_df.show(2)\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 64,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+---------------+----------+-----+------+---+-----+----------+------------------+---------------+\n|              emp_id|  employee_name|department|state|salary|age|bonus|        ts|             email|    credit_card|\n+--------------------+---------------+----------+-----+------+---+-----+----------+------------------+---------------+\n|9754f7a8-5e59-485...|Garrett Stewart|     Sales|   CA| 61400| 32|28970|1044552062|gina03@example.org|348909523317140|\n|ccce94cc-ad7f-4d2...|Ronald Campbell| Marketing|   TX| 46067| 27|45654| 995679202| qpark@example.com|349250521614603|\n+--------------------+---------------+----------+-----+------+---+-----+----------+------------------+---------------+\nonly showing top 2 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Creating Hudi Tables ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"hudi_table\"\n\nrecordkey = 'emp_id'\npath = \"s3://soumilshah-hudi-demos/tmp/pii/\"\n\nmethod = 'upsert'\ntable_type = \"COPY_ON_WRITE\"\n\nconnection_options={\n    \"path\": path,\n    \"connectionName\": \"hudi-connection\",\n\n    \"hoodie.datasource.write.storage.type\": table_type,\n    'className': 'org.apache.hudi',\n    'hoodie.table.name': table_name,\n    'hoodie.datasource.write.recordkey.field': recordkey,\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': method,\n    \n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': db_name,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 65,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "WriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 66,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# lets see how we can Mask PII data which is already on Datalake ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from awsglueml.transforms import EntityDetector\nMASK = \"#########\"\nPII = [\n        \"CREDIT_CARD\",\n        \"EMAIL\"\n    ]\n\nglueDf  = (\n    glueContext.create_dynamic_frame.from_options(\n        connection_type=\"marketplace.spark\",\n        connection_options={\n            \"path\": path,\n            \"connectionName\": \"hudi-connection\",\n        },\n        transformation_ctx=\"glueDf\",\n    )\n)\n\nentity_detector = EntityDetector()\n\ndetected_df = entity_detector.detect(\n    glueDf,\n    PII,\n    \"DetectedEntities\",\n)\n\n\ndef replace_cell(original_cell_value, sorted_reverse_start_end_tuples):\n    if sorted_reverse_start_end_tuples:\n        for entity in sorted_reverse_start_end_tuples:\n            to_mask_value = original_cell_value[entity[0] : entity[1]]\n            original_cell_value = original_cell_value.replace(\n                to_mask_value, MASK\n            )\n    return original_cell_value\n\n\ndef row_pii(column_name, original_cell_value, detected_entities):\n    if column_name in detected_entities.keys():\n        entities = detected_entities[column_name]\n        start_end_tuples = map(\n            lambda entity: (entity[\"start\"], entity[\"end\"]), entities\n        )\n        sorted_reverse_start_end_tuples = sorted(\n            start_end_tuples, key=lambda start_end: start_end[1], reverse=True\n        )\n        return replace_cell(original_cell_value, sorted_reverse_start_end_tuples)\n    return original_cell_value\n\n\nrow_pii_udf = udf(row_pii, StringType())\n\n\ndef recur(df, remaining_keys):\n    if len(remaining_keys) == 0:\n        return df\n    else:\n        head = remaining_keys[0]\n        tail = remaining_keys[1:]\n        modified_df = df.withColumn(\n            head, row_pii_udf(lit(head), head, \"DetectedEntities\")\n        )\n        return recur(modified_df, tail)\n\n\nkeys = glueDf.toDF().columns\nupdated_masked_df = recur(detected_df.toDF(), keys)\nupdated_masked_df = updated_masked_df.drop(\"DetectedEntities\")\n\nDetectSensitiveData_node1674307865885 = DynamicFrame.fromDF(\n    updated_masked_df, glueContext, \"updated_masked_df\"\n)\nDetectSensitiveData_df = DetectSensitiveData_node1674307865885.toDF()\n\ncolumn_process = [column for column in DetectSensitiveData_df.columns if column not in \n                  ['_hoodie_commit_time', '_hoodie_commit_seqno', '_hoodie_record_key', '_hoodie_partition_path', '_hoodie_file_name']]\n",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 69,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "DetectSensitiveData_df.select(column_process).show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 70,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+---------------+----------+-----+------+---+-----+----------+---------+-----------+\n|              emp_id|  employee_name|department|state|salary|age|bonus|        ts|    email|credit_card|\n+--------------------+---------------+----------+-----+------+---+-----+----------+---------+-----------+\n|9754f7a8-5e59-485...|Garrett Stewart|     Sales|   CA| 61400| 32|28970|1044552062|#########|  #########|\n|15ddbe57-47ad-499...|   Leslie Payne| Marketing|   IL| 96788| 20|72432|1663487013|#########|  #########|\n|43ea4c7f-67a5-4bf...| Sandra Camacho|        HR|   CA| 52474| 54|98622| 185484648|#########|  #########|\n|1cefb593-a8f0-47c...|  Allison Davis| Marketing|   CA|124597| 38|77046|1442842439|#########|  #########|\n|66713ca4-d011-4b5...|       Ryan Cox| Marketing|   IL|110613| 51|55816| 191214027|#########|  #########|\n+--------------------+---------------+----------+-----+------+---+-----+----------+---------+-----------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Write Masked data back intop Hudi tables ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "processed_df = DetectSensitiveData_df.select(column_process)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 71,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "processed_df.show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 72,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+---------------+----------+-----+------+---+-----+----------+---------+-----------+\n|              emp_id|  employee_name|department|state|salary|age|bonus|        ts|    email|credit_card|\n+--------------------+---------------+----------+-----+------+---+-----+----------+---------+-----------+\n|9754f7a8-5e59-485...|Garrett Stewart|     Sales|   CA| 61400| 32|28970|1044552062|#########|  #########|\n|15ddbe57-47ad-499...|   Leslie Payne| Marketing|   IL| 96788| 20|72432|1663487013|#########|  #########|\n|43ea4c7f-67a5-4bf...| Sandra Camacho|        HR|   CA| 52474| 54|98622| 185484648|#########|  #########|\n+--------------------+---------------+----------+-----+------+---+-----+----------+---------+-----------+\nonly showing top 3 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Write Masked Data into Hudi table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "\nWriteDF = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(processed_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"processed_df\",\n    )\n)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 73,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}